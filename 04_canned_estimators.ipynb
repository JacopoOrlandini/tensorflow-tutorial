{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canned Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "  * https://github.com/random-forests/tensorflow-workshop/blob/master/examples/04_canned_estimators.ipynb\n",
    "  * https://www.tensorflow.org/programmers_guide/estimators\n",
    "  * https://www.tensorflow.org/get_started/estimator\n",
    "\n",
    "In this notebook we'll demonstrate how to use two Canned Estimators (\"models-in-a-box\").\n",
    "[TensorFlow Estimators](https://www.tensorflow.org/get_started/estimator) provide a high-level API loosely inspired by [scikit-learn](http://scikit-learn.org) to train your models. These encapsulate the lower-level TensorFlow code we've seen so far, so you can focus on solving your problem. There are several advantages to Canned Estimators.\n",
    "\n",
    "* Estimators build the graph for you â€“ you don't have to build the graph.\n",
    "\n",
    "\n",
    "* You won't have to manage Sessions, or write your own logic for TensorBoard, or for saving and loading checkpoints.\n",
    "\n",
    "\n",
    "* You'll get out-of-the-box distributed training (of course, you will have to take care to read your data efficiently, and set up a cluster).\n",
    "\n",
    "\n",
    "Here's a diagram of the methods we'll use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/random-forests/tensorflow-workshop/master/images/estimators1.jpeg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url='https://raw.githubusercontent.com/random-forests/tensorflow-workshop/master/images/estimators1.jpeg',\n",
    "      width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can probably guess the purpose of methods like train / evaluate / and predict. What may be new to you, though, are [Input Functions](https://www.tensorflow.org/get_started/estimator#describe_the_training_input_pipeline). These are responsible for reading your data, preprocessing it, and sending it to the model. When you use an input function, your code will read ``estimator.train(your_input_function)`` rather than ``estimator.train(your_training_data)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using MNIST dataset, but in contrast to before, each label corresponds to a single integer instead of a vector (one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# It will be downloaded to './data' if you don't already have a local copy.\n",
    "mnist = input_data.read_data_sets('./data', one_hot=False)\n",
    "\n",
    "images_train = mnist.train.images\n",
    "labels_train = mnist.train.labels.astype(np.int32)\n",
    "images_test = mnist.test.images\n",
    "labels_test = mnist.test.labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``tf.estimator`` API uses input functions, which create the TensorFlow operations that generate data for the model. These provide batching and other features for you, so you don't have to write that code yourself. Here, we'll read data using [tf.estimator.inputs.numpy_input_fn](https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn), which is appropriate for in-memory data. For large datasets that do not fit in memory, the new [Dataset API](https://www.tensorflow.org/programmers_guide/datasets) is preferred.\n",
    "\n",
    "We'll define one function to import the training set and another function to import the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'x': images_train},\n",
    "    labels_train, \n",
    "    num_epochs=None, # repeat forever\n",
    "    shuffle=True     # shuffle the training data \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'x': images_test},\n",
    "    labels_test,\n",
    "    num_epochs=1, # loop through the dataset once\n",
    "    shuffle=False # don't shuffle the test data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need two input functions? There are a couple differences in how we handle our training and testing data. We want the training input function to loop over the data indefinitely (returning batches of examples and labels when called). We want the testing input function run for just one epoch, so we can make one prediction for each testing example. We'll also want to shuffle the training data, but not the testing data (so we can compare it to the labels later).\n",
    "\n",
    "* By setting ```num_epochs``` to ```None```, we'll loop over the data indefinitely so we can train for as long as we like.\n",
    "* The default ```batch_size``` is ```128```, but you can provide a different parameter if you like.\n",
    "\n",
    "You can read more about the numpy input function [here](https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn). Alternatively, you can use an [input function](https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/pandas_input_fn) that can process ``pandas.DataFrame``s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feature Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define the model's feature columns, which specify the data type for the features in the data set. All the features are continuous, so ``tf.feature_column.numeric_column`` is the appropriate function to use to construct the feature columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the names must match the input function\n",
    "feature_columns = [tf.feature_column.numeric_column('x', shape=784)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate pre-made Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll create a [LinearClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier) - this is identical to our Softmax (aka, multiclass logistic regression model) from the third notebok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './graphs/04_canned/linear', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.LinearClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    n_classes=10,\n",
    "    model_dir=\"./graphs/04_canned/linear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code creates a ``LinearClassifier`` model using the following arguments:\n",
    "\n",
    "  * ``feature_columns=feature_columns``: The set of feature columns defined above.  \n",
    "  * ``n_classes=10``: The target classes, representing the digits.\n",
    "  * ``model_dir=\"./graphs/04_canned/linear\"``: The directory in which TensorFlow will save checkpoint data and TensorBoard summaries during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./graphs/04_canned/linear/model.ckpt-2000\n",
      "INFO:tensorflow:Saving checkpoints for 2001 into ./graphs/04_canned/linear/model.ckpt.\n",
      "INFO:tensorflow:loss = 32.1833, step = 2001\n",
      "INFO:tensorflow:global_step/sec: 598.619\n",
      "INFO:tensorflow:loss = 33.8894, step = 2101 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 701.149\n",
      "INFO:tensorflow:loss = 34.0114, step = 2201 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 654.416\n",
      "INFO:tensorflow:loss = 40.7652, step = 2301 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 604.043\n",
      "INFO:tensorflow:loss = 36.6944, step = 2401 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 667.478\n",
      "INFO:tensorflow:loss = 15.3372, step = 2501 (0.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 690.35\n",
      "INFO:tensorflow:loss = 20.87, step = 2601 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 673.858\n",
      "INFO:tensorflow:loss = 24.9973, step = 2701 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 676.169\n",
      "INFO:tensorflow:loss = 47.8588, step = 2801 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.133\n",
      "INFO:tensorflow:loss = 23.4726, step = 2901 (0.167 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into ./graphs/04_canned/linear/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 26.9286.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x11ba02e48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I've arbitrarily decided to train for 1000 steps\n",
    "estimator.train(input_fn=train_input, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-10-03-21:14:45\n",
      "INFO:tensorflow:Restoring parameters from ./graphs/04_canned/linear/model.ckpt-3000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-03-21:14:46\n",
      "INFO:tensorflow:Saving dict for global step 3000: accuracy = 0.9252, average_loss = 0.273583, global_step = 3000, loss = 34.6307\n",
      "{'accuracy': 0.92519999, 'average_loss': 0.27358285, 'loss': 34.630737, 'global_step': 3000}\n"
     ]
    }
   ],
   "source": [
    "# We should see about 90% accuracy here.\n",
    "evaluation = estimator.evaluate(input_fn=test_input)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Estimator`` returns a generator object. This bit of code demonstrates how to retrieve predictions for individual examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./graphs/04_canned/linear/model.ckpt-3000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAACPCAYAAAB9P0c7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127c5dc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This returns a generator object\n",
    "predictions = estimator.predict(input_fn=test_input)\n",
    "\n",
    "# how many digits we will display\n",
    "n = 10\n",
    "\n",
    "i = 0\n",
    "plt.figure(figsize=(20, 4))\n",
    "for p in predictions:\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    ax.imshow(images_test[i].reshape(28, 28), cmap=plt.cm.gray_r)\n",
    "    actual = labels_test[i]\n",
    "    predicted = p['class_ids'][0]\n",
    "    ax.set_title(\"Predicted: %d\\nActual: %d\" % (predicted, actual))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    i += 1\n",
    "    if i == n:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``tf.estimator`` offers a variety of predefined models. Here, we'll configure a *Deep Neural Network Classifier* model using the provided [DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) class. We supply all the configuration parameters right in the constructor, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './graphs/04_canned/deep', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[256, 256], # we will arbitrarily use two hidden layer\n",
    "    n_classes=10,\n",
    "    model_dir=\"./graphs/04_canned/deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how easy it is to switch the model to a fully connected DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./graphs/04_canned/deep/model.ckpt-4000\n",
      "INFO:tensorflow:Saving checkpoints for 4001 into ./graphs/04_canned/deep/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.988206, step = 4001\n",
      "INFO:tensorflow:global_step/sec: 151.737\n",
      "INFO:tensorflow:loss = 0.55815, step = 4101 (0.660 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.458\n",
      "INFO:tensorflow:loss = 2.24853, step = 4201 (0.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.656\n",
      "INFO:tensorflow:loss = 0.795035, step = 4301 (0.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.701\n",
      "INFO:tensorflow:loss = 2.56438, step = 4401 (0.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.221\n",
      "INFO:tensorflow:loss = 1.87555, step = 4501 (0.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.595\n",
      "INFO:tensorflow:loss = 2.76659, step = 4601 (0.675 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.876\n",
      "INFO:tensorflow:loss = 0.963839, step = 4701 (0.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.463\n",
      "INFO:tensorflow:loss = 1.8605, step = 4801 (0.643 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.297\n",
      "INFO:tensorflow:loss = 0.929676, step = 4901 (0.636 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.249\n",
      "INFO:tensorflow:loss = 0.849985, step = 5001 (0.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.043\n",
      "INFO:tensorflow:loss = 1.53895, step = 5101 (0.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.309\n",
      "INFO:tensorflow:loss = 1.27397, step = 5201 (0.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.833\n",
      "INFO:tensorflow:loss = 0.730474, step = 5301 (0.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 136.464\n",
      "INFO:tensorflow:loss = 0.624911, step = 5401 (0.734 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.67\n",
      "INFO:tensorflow:loss = 1.31197, step = 5501 (0.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.521\n",
      "INFO:tensorflow:loss = 1.47872, step = 5601 (0.717 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.081\n",
      "INFO:tensorflow:loss = 3.50435, step = 5701 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.229\n",
      "INFO:tensorflow:loss = 0.93678, step = 5801 (0.718 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.898\n",
      "INFO:tensorflow:loss = 0.699645, step = 5901 (0.633 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6000 into ./graphs/04_canned/deep/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.25849.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x11f9bf4a8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I've arbitrarily decided to train for 2000 steps\n",
    "estimator.train(train_input, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-10-03-21:15:48\n",
      "INFO:tensorflow:Restoring parameters from ./graphs/04_canned/deep/model.ckpt-6000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-03-21:15:49\n",
      "INFO:tensorflow:Saving dict for global step 6000: accuracy = 0.9764, average_loss = 0.0935217, global_step = 6000, loss = 11.8382\n",
      "{'accuracy': 0.97640002, 'average_loss': 0.09352167, 'loss': 11.838185, 'global_step': 6000}\n"
     ]
    }
   ],
   "source": [
    "# Expect accuracy around 97%\n",
    "evaluation = estimator.evaluate(input_fn=test_input)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can start TensorBoard by running this from a terminal command (in the same directory as this notebook):\n",
    "\n",
    "``` $ tensorboard --logdir=./graphs/04_canned/ ```\n",
    "\n",
    "then pointing your web-browser to http://localhost:6006 (check the TensorBoard output in the terminal in case it's running on a different port).\n",
    "\n",
    "When that launches, you'll be able to see a variety of graphs that compares the linear and deep models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Although the Estimators we used here are relative simple (a LinearClassifier, and a Fully Connected Deep Neural Network), TensorFlow also provides more interesting ones (e.g. a [Wide and Deep model](https://www.tensorflow.org/tutorials/wide_and_deep).\n",
    "\n",
    "To learn more about Estimators, you can watch this talk from Google I/O by Martin Wicke: [Effective TensorFlow for Non-Experts](https://www.youtube.com/watch?v=5DknTFbcGVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
