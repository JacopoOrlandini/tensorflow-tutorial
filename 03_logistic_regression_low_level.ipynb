{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "  1. https://www.tensorflow.org/get_started/mnist/beginners\n",
    "  2. https://www.tensorflow.org/get_started/mnist/pros\n",
    "  3. https://github.com/random-forests/tensorflow-workshop/blob/master/examples/02_logistic_regression_low_level.ipynb\n",
    "\n",
    "The [MNIST dataset](http://yann.lecun.com/exdb/mnist) consists of handwritten digit images and it is divided in 60,000 examples for the training set and 10,000 examples for testing. In many papers as well as in this tutorial, the official training set of 60,000 is divided into an actual training set of 50,000 examples and 10,000 validation examples (for selecting hyper-parameters like learning rate and size of the model). All digit images have been size-normalized and centered in a fixed size image of 28 x 28 pixels. In the original dataset each pixel of the image is represented by a value between 0 and 255, where 0 is black, 255 is white and anything in between is a different shade of grey.\n",
    "\n",
    "Let's start by download the dataset and inspecting some of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# It will be downloaded to './data' if you don't already have a local copy.\n",
    "mnist = input_data.read_data_sets('./data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is split into three parts: 55,000 data points of training data (``mnist.train``), 10,000 points of test data (``mnist.test``), and 5,000 points of validation data (``mnist.validation``). This split is very important: it's essential in machine learning that we have separate data which we don't learn from so that we can make sure that what we've learned actually generalizes!\n",
    "\n",
    "As mentioned earlier, every MNIST data point has two parts: an image of a handwritten digit and a corresponding label. We'll call the images \"x\" and the labels \"y\". Both the training set and test set contain images and their corresponding labels; for example the training images are ``mnist.train.images`` and the training labels are ``mnist.train.labels``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, validation, test: 55000, 5000, 10000\n"
     ]
    }
   ],
   "source": [
    "print ('Train, validation, test: %d, %d, %d' %\n",
    "      (len(mnist.train.images), len(mnist.validation.images), len(mnist.test.images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is 28 pixels by 28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAABzCAYAAAAfb55ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAAAAH/g/vwlX2M7Y/VkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1203d82b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    ax.imshow(mnist.train.images[i].reshape(28, 28), cmap=plt.cm.gray_r)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can flatten this array into a vector of 28x28 = 784 numbers. It doesn't matter how we flatten the array, as long as we're consistent between images. From this perspective, the MNIST images are just a bunch of points in a 784-dimensional vector space.\n",
    "\n",
    "The result is that ``mnist.train.images`` is a tensor (an n-dimensional array) with a shape of ``[55000, 784]``. The first dimension is an index into the list of images and the second dimension is the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image.\n",
    "\n",
    "For the purposes of this tutorial, we're going to want our labels as \"one-hot vectors\". A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the\n",
    "th digit will be represented as a vector which is 1 in the th dimension.\n",
    "For example, 3 would be ``[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]``. Consequently, ``mnist.train.labels`` is a ``[55000, 10]`` array of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that every image in MNIST is of a handwritten digit between zero and nine. So there are only ten possible things that a given image can be. We want to be able to look at an image and give the probabilities for it being each digit. For example, our model might look at a picture of a nine and be 80% sure it's a nine, but give a 5% chance to it being an eight (because of the top loop) and a bit of probability to all the others because it isn't 100% sure.\n",
    "\n",
    "The setup will be similar to the previous notebook on linear regression. The principal changes are:\n",
    "* We have switched from regression to classification.\n",
    "* We are using a different loss function. Instead of using squared error, we will now use cross-entropy.\n",
    "* We are using a new dataset. MNIST contains 28x28 pixel handwritten digits.\n",
    "\n",
    "An important takeaway: notice that despite these changes, the line that creates the gradient descent optimizer is identical to the previous notebook. This is the magic of automatic differentiation. Once we've specified our graph and the loss function, TensorFlow is able to analyze it for us, and determine how to adjust our variables to decrease the loss.\n",
    "\n",
    "The model we train here is unimpressive in terms of accuracy. The goal is to introduce you to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a new ``tf.Session``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants we will be using later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_PIXELS = 28 * 28\n",
    "TRAIN_STEPS = 2000\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining placeholders for the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = tf.placeholder(dtype=tf.float32, shape=[None, NUM_PIXELS])\n",
    "labels = tf.placeholder(dtype=tf.float32, shape=[None, NUM_CLASSES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to input any number of MNIST images, each flattened into a 784-dimensional vector. We represent this as a 2-D tensor of floating-point numbers, with a shape ``[None, 784]`` (Here ``None`` means that a dimension can be of any length.)\n",
    "\n",
    "Next, we'll define the weights and intercepts for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('linear_predictor'):\n",
    "    W = tf.Variable(tf.truncated_normal([NUM_PIXELS, NUM_CLASSES]))\n",
    "    b = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "    µ = tf.matmul(images, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that ``W`` has a shape of ``[784, 10]`` because we want to multiply the 784-dimensional image vectors by it to produce 10-dimensional vectors of evidence for the different classes. ``b`` has a shape of ``[10]`` so we can add it to the output. We initialize the weights ``W`` with 2-D matrix generated by ``tf.truncated_normal``, which generates a random distribution with mean zero and unit standard deviation. The intercepts ``b`` are initialized with a 1-D vector of all zeros.\n",
    "\n",
    "Finally, our linear predictor is $\\mu_i = \\mathbf{W} \\mathbf{x}_i + b$, where $\\mathbf{x}$ is the 784-dimensional vector of the $i$-th image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our model, we need to define what it means for the model to be good. Well, actually, in machine learning we typically define what it means for a model to be bad. We call this the *cost*, or the *loss*, and it represents how far off our model is from our desired outcome. We try to minimize that error, and the smaller the error margin, the better our model is.\n",
    "\n",
    "One very common, very nice function to determine the loss of a model is called **cross-entropy**. Cross-entropy arises from thinking about information compressing codes in information theory but it winds up being an important idea in lots of areas, from gambling to machine learning. It's defined as:\n",
    "$$\n",
    "H_{y'}(y) = -\\sum_i y'_i \\log(y_i)\n",
    "$$\n",
    "\n",
    "Where $y$ is our predicted probability distribution, and $y^\\prime$ is the true distribution (the one-hot vector with the digit labels). In some rough sense, the cross-entropy is measuring how inefficient our predictions are for describing the truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the cross-entropy function as\n",
    "```python\n",
    "y = tf.nn.softmax(µ)\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(labels * tf.log(y), reduction_indices=[1]))\n",
    "```\n",
    "\n",
    "First, ``tf.log`` computes the logarithm of each element of ``y``. Next, we multiply each element of ``labels`` with the corresponding element of ``tf.log(y)``. Then ``tf.reduce_sum`` adds the elements in the second dimension of ``y``, due to the ``reduction_indices=[1]`` parameter. Finally, ``tf.reduce_mean`` computes the mean over all the examples in the batch.\n",
    "\n",
    "Note that in the source code, we don't use this formulation, because it is numerically unstable. Instead, we apply ``tf.nn.softmax_cross_entropy_with_logits`` on the unnormalized logits (e.g., we call ``softmax_cross_entropy_with_logits`` on ``µ``, because this more numerically stable function internally computes the softmax activation. In your code, consider using ``tf.nn.softmax_cross_entropy_with_logits`` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=µ, labels=labels))\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our model do?\n",
    "\n",
    "Well, first let's figure out where we predicted the correct label. ``tf.argmax`` is an extremely useful function which gives you the index of the highest entry in a tensor along some axis. For example, ``tf.argmax(y,1)`` is the label our model thinks is most likely for each input, while ``tf.argmax(y_,1)`` is the correct label. We can use ``tf.equal`` to check if our prediction matches the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('evaluation'):\n",
    "    prediction = tf.argmax(µ, 1)\n",
    "    predicted_vs_actual = tf.equal(prediction, tf.argmax(labels, 1))\n",
    "    model_accuracy = tf.reduce_mean(tf.cast(predicted_vs_actual, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``predicted_vs_actual`` us a list of booleans. To determine what fraction are correct, we cast to floating point numbers and then take the mean (``model_accuracy``). For example, ``[True, False, True, True]`` would become ``[1,0,1,1]`` which would become 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the traning, we setup summary statistics to be visualized in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the graph\n",
    "LOGDIR = './graphs'\n",
    "\n",
    "writer = tf.summary.FileWriter(join(LOGDIR, '02_logistic_regression'))\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll be using [tf.summary.histogram](https://www.tensorflow.org/get_started/tensorboard_histograms), which constructs a histogram of the weights ``W`` in each iteration of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.histogram('weights', W)\n",
    "    tf.summary.scalar('accuracy', model_accuracy)\n",
    "\n",
    "# This op will calculate our summary data when run\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train – we'll run the training step 2000 times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(TRAIN_STEPS):\n",
    "    batch_images, batch_labels = mnist.train.next_batch(BATCH_SIZE)\n",
    "    summary_result, _ = sess.run([summary_op, train_step],\n",
    "                                 feed_dict={images: batch_images, labels: batch_labels})\n",
    "\n",
    "    # write the summary data to disk\n",
    "    writer.add_summary(summary_result, i)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each step of the loop, we get a \"batch\" of one hundred random data points from our training set. We run ``train_step`` feeding in the batches data to replace the placeholders.\n",
    "\n",
    "Using small batches of random data is called stochastic training – in this case, stochastic gradient descent. Ideally, we'd like to use all our data for every step of training because that would give us a better sense of what we should be doing, but that's expensive. So, instead, we use a different subset every time. Doing this is cheap and has much of the same benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the trained model on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.900100\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy %f\" % sess.run(model_accuracy, feed_dict={images: mnist.test.images, \n",
    "                                                    labels: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be about 90-92%.\n",
    "\n",
    "Is that good? Well, not really. In fact, it's pretty bad. This is because we're using a very simple model. With some small changes, we can get to 97%. The best models can get to over 99.7% accuracy! (For more information, have a look at this [list of results](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As written, this code evaluates the accuracy of the trained model on the entire testing set. Below is a function to predict the label for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(i):\n",
    "    image = mnist.test.images[i]\n",
    "    actual_label = np.argmax(mnist.test.labels[i])\n",
    "    predicted_label = sess.run(prediction, feed_dict={images: [image]})\n",
    "    return predicted_label, actual_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAACPCAYAAAB9P0c7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12db09358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    ax.imshow(mnist.test.images[i].reshape(28, 28), cmap=plt.cm.gray_r)\n",
    "    predicted, actual = predict(i)\n",
    "    ax.set_title(\"Predicted: %d\\nActual: %d\" % (predicted, actual))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
